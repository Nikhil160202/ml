Q.1. Write a python program to categorize the given news text into one of the available categories of news groups, using multinomial NaÃ¯ve Bayes machine learning model.
Ans:-
import os
import string
import numpy as np
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
X = [] # an element of X is represented as (filename,text)
Y = [] # an element of Y represents the newsgroup category of the
corresponding X element
for category in os.listdir('C:/Users/PC 04/Downloads/20_newsgroups'):
for document in os.listdir('C:/Users/PC
04/Downloads/20_newsgroups/'+category):
with open('C:/Users/PC
04/Downloads/20_newsgroups/'+category+'/'+document, "r") as f:
X.append((document,f.read()))
Y.append(category)
X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y,
test_size=0.25, random_state=0)
# Building a vocabulary of words from the given documents
vocab = {}
for i in range(len(X_train)):
word_list = []
for word in X_train[i][1].split():
word_new = word.strip(string.punctuation).lower()
if (len(word_new)>2) and (word_new not in stopwords):
if word_new in vocab:
vocab[word_new]+=1
else:
vocab[word_new]=1
# Plotting a graph of no of words with a given frequency to decide cutoff
drequency
num_words = [0 for i in range(max(vocab.values())+1)]
freq = [i for i in range(max(vocab.values())+1)]
for key in vocab:
num_words[vocab[key]]+=1
plt.plot(freq,num_words)
plt.axis([1, 10, 0, 20000])
plt.xlabel("Frequency")
plt.ylabel("No of words")
plt.grid()
plt.show()
cutoff_freq = 80
# For deciding cutoff frequency
num_words_above_cutoff = len(vocab)-sum(num_words[0:cutoff_freq])
print("Number of words with frequency higher than cutoff frequency({})
:".format(cutoff_freq),num_words_above_cutoff)
# Words with frequency higher than cutoff frequency are chosen as features
# (i.e we remove words with low frequencies as they would not be
significant )
features = []
for key in vocab:
if vocab[key] >=cutoff_freq:
features.append(key)
# To represent training data as word vector counts
X_train_dataset = np.zeros((len(X_train),len(features)))
# This can take some time to complete
for i in range(len(X_train)):
# print(i) # Uncomment to see progress
word_list = [ word.strip(string.punctuation).lower() for word in
X_train[i].split()]
for word in word_list:
if word in features:
X_train_dataset[i][features.index(word)] += 1
# To represent test data as word vector counts
X_test_dataset = np.zeros((len(X_test),len(features)))
# This can take some time to complete
for i in range(len(X_test)):
# print(i) # Uncomment to see progress
word_list = [ word.strip(string.punctuation).lower() for word in
X_test[i][1].split()]
for word in word_list:
if word in features:
X_test_dataset[i][features.index(word)] += 1
# Using sklearn's Multinomial Naive Bayes
clf = MultinomialNB()
clf.fit(X_train_dataset,Y_train)
Y_test_pred = clf.predict(X_test_dataset)
sklearn_score_train = clf.score(X_train_dataset,Y_train)
print("Sklearn's score on training data :",sklearn_score_train)
sklearn_score_test = clf.score(X_test_dataset,Y_test)
print("Sklearn's score on testing data :",sklearn_score_test)
print("Classification report for testing data :-")
print(classification_report(Y_test, Y_test_pred))
# Implementing Multinomial Naive Bayes from scratch
class MultinomialNaiveBayes:
def __init__(self)
self.count = {}
# classes represents the different news categories
self.classes = None
def fit(self,X_train,Y_train):
# This can take some time to complete
self.classes = set(Y_train)
for class_ in self.classes:
self.count[class_] = {}
for i in range(len(X_train[0])):
self.count[class_][i] = 0
self.count[class_]['total'] = 0
self.count[class_]['total_points'] = 0
self.count['total_points'] = len(X_train)
for i in range(len(X_train)):
for j in range(len(X_train[0])):
self.count[Y_train[i]][j]+=X_train[i][j]
self.count[Y_train[i]]['total']+=X_train[i][j]
self.count[Y_train[i]]['total_points']+=1
def __probability(self,test_point,class_):
log_prob = np.log(self.count[class_]['total_points']) -
np.log(self.count['total_points'])
total_words = len(test_point)
for i in range(len(test_point)):
current_word_prob =
test_point[i]*(np.log(self.count[class_][i]+1)-
np.log(self.count[class_]['total']+total_words))
log_prob += current_word_prob
return log_prob
def __predictSinglePoint(self,test_point):
best_class = None
best_prob = None
first_run = True
for class_ in self.classes:
log_probability_current_class =
self.__probability(test_point,class_)
if (first_run) or (log_probability_current_class > best_prob) :
best_class = class_
best_prob = log_probability_current_class
first_run = False
return best_class
def predict(self,X_test):
# This can take some time to complete
Y_pred = []
for i in range(len(X_test)):
# print(i)
# Uncomment to see progress
Y_pred.append( self.__predictSinglePoint(X_test[i]) )
return Y_pred
def score(self,Y_pred,Y_true):
# returns the mean accuracy
count = 0
for i in range(len(Y_pred)):
if Y_pred[i] == Y_true[i]:
count+=1
return count/len(Y_pred)
clf2 = MultinomialNaiveBayes()
clf2.fit(X_train_dataset,Y_train)
Y_test_pred = clf2.predict(X_test_dataset)
our_score_test = clf2.score(Y_test_pred,Y_test)
print("Our score on testing data :",our_score_test)
print("Classification report for testing data :-")
print(classification_report(Y_test, Y_test_pred))
print("Score of our model on test data:",our_score_test)
print("Score of inbuilt sklearn's MultinomialNB on the same data:",sklearn_score_test)


Q.2. Write a python program to implement Decision Tree whether or not to play Tennis.
Ans
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
# Step 1: Create the dataset (if it's not loaded from CSV)
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast', 'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Mild', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild', 'Mild', 'Hot', 'Mild', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Windy': ['False', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False', 'False', 'True', 'True', 'False', 'False'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}
# Step 2: Convert the data to a DataFrame
df = pd.DataFrame(data)
# Step 3: Preview the dataset
print("\nDataset Preview:")
print(df.head())
# Step 4: Encode categorical variables to numeric values using LabelEncoder
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
# Encode the categorical columns
df['Outlook'] = label_encoder.fit_transform(df['Outlook'])
df['Temperature'] = label_encoder.fit_transform(df['Temperature'])
df['Humidity'] = label_encoder.fit_transform(df['Humidity'])
df['Windy'] = label_encoder.fit_transform(df['Windy'])
df['PlayTennis'] = label_encoder.fit_transform(df['PlayTennis'])
# Step 5: Split the dataset into features (X) and target (y)
X = df.drop('PlayTennis', axis=1)  # Features (independent variables)
y = df['PlayTennis']  # Target variable (dependent variable)
# Step 6: Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Step 7: Initialize the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
# Step 8: Train the model
dt_model.fit(X_train, y_train)
# Step 9: Make predictions on the test set
y_pred = dt_model.predict(X_test)
# Step 10: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)
# Step 11: Print the results
print(f"\nAccuracy: {accuracy}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)
# Displaying actual vs predicted values
print("\nActual vs Predicted values:")
for i in range(len(y_test)):
    print(f"Actual: {y_test.iloc[i]}, Predicted: {y_pred[i]}")
